{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models.doc2vec import TaggedDocument,Doc2Vec\n",
    "from cold_start_d2v.create_d2v import create_doc2vec_model\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial,stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc2vec_model(articles,name,vector_size=100,epochs=10,lang='en'):\n",
    "    # import stopwords for specific language of model\n",
    "    stop_words = get_stop_words(lang)\n",
    "    ## list of just articles (str)\n",
    "    #strip stopwords article docs\n",
    "    nostop = [[i for i in doc.lower().split() if i not in stop_words] for doc in articles]\n",
    "    #tokenize article docs and convert to doc2vec tagged docs - each article has an index number and list of tokens - taggedoc(['token1','token2',[1]])\n",
    "    tagged = [TaggedDocument(doc,[i]) for i,doc in enumerate(nostop)]\n",
    "    # instantiate doc2vec model with parameters - size = # of nums representing each doc (100), min_count - occurences of words in vocab (filter out rare words), iter - passes to create vectors\n",
    "    model = Doc2Vec(vector_size=vector_size, min_count=2, epochs=epochs)\n",
    "    ## build vocab from all tagged docs\n",
    "    model.build_vocab(tagged)\n",
    "    ## train model on tagged docs - total examples - total # of docs\n",
    "    model.train(tagged,total_examples=model.corpus_count,epochs=epochs)\n",
    "    # save model with language - eg esmodel.model for spanish docs\n",
    "    model_name = name + 'model.model'\n",
    "    model.save(model_name)\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    print('saved as: ' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(textlist,lang,d2v_model):\n",
    "    stop_words = get_stop_words(lang)\n",
    "    histnostop = [[i for i in doc.lower().split() if i not in stop_words] for doc in textlist]\n",
    "    dlhist_tagged = [TaggedDocument(doc,[i]) for i,doc in enumerate(histnostop)]\n",
    "    ## infer vectors from current doc2model\n",
    "    vecs = [d2v_model.infer_vector(doc.words) for doc in dlhist_tagged]\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotions(text_list,lang,emotion_vecs,model_name):\n",
    "    d2v_model = Doc2Vec.load(model_name)\n",
    "    text_vecs = text2vec(text_list,lang,d2v_model)\n",
    "    emot_scan = [[1 - spatial.distance.cosine(sample_vec,x) for x in emot_vecs]for sample_vec in text_vecs]\n",
    "    emot_pred = [np.argmax(x) for x in emot_scan]\n",
    "    return emot_scan,emot_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isear = pd.read_csv('ISEAR.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_id = []\n",
    "for x in df_isear['emotion']:\n",
    "    if x == 'anger':\n",
    "        emotion_id.append(0)\n",
    "    if x == 'disgust':\n",
    "        emotion_id.append(1)\n",
    "    if x == 'fear':\n",
    "        emotion_id.append(2)\n",
    "    if x == 'guilt':\n",
    "        emotion_id.append(3)\n",
    "    if x == 'joy':\n",
    "        emotion_id.append(4)\n",
    "    if x == 'sadness':\n",
    "        emotion_id.append(5)\n",
    "    if x == 'shame':\n",
    "        emotion_id.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isear['id'] = emotion_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               text  id\n",
       "0      joy  On days when I feel close to my partner and ot...   4\n",
       "1     fear  Every time I imagine that someone I love or I ...   2\n",
       "2    anger  When I had been obviously unjustly treated and...   0\n",
       "3  sadness  When I think about the short time that we live...   5\n",
       "4  disgust  At a gathering I found myself involuntarily si...   1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_text = df_isear['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: emotionsmodel.model\n"
     ]
    }
   ],
   "source": [
    "create_doc2vec_model(emot_text ,'emotions',vector_size=100,epochs=10,lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load('emotionsmodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_vecs = [d2v_model.docvecs[x] for x in range(len(emot_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isear['emot_vecs'] = emot_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_grp = list(df_isear.groupby(df_isear['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotions_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_vec_mean = [{'emotion':emotions_grp[x][0],'vec':emotions_grp[x][1]['emot_vecs'].mean()} for x in range(len(emotions_grp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03394571,  0.01575613, -0.02299005,  0.00400032, -0.00243837,\n",
       "       -0.02370932,  0.02524502,  0.01001316,  0.03191565,  0.03928171,\n",
       "       -0.020799  ,  0.00927222, -0.02194257, -0.03660933,  0.02896129,\n",
       "        0.00516501,  0.02101598,  0.04501959, -0.02352156,  0.0122413 ,\n",
       "       -0.01742102,  0.00820629, -0.00634036, -0.02798223, -0.01981252,\n",
       "        0.00572883, -0.00220514, -0.00909239,  0.00695971, -0.0202946 ,\n",
       "       -0.01773105,  0.0097519 ,  0.01678837, -0.00122372, -0.03114676,\n",
       "        0.02916909,  0.01618617,  0.01141875,  0.01230535, -0.01458297,\n",
       "       -0.00460585,  0.00677788, -0.00685136, -0.01285849, -0.01681715,\n",
       "       -0.01826535, -0.01752163,  0.01298876, -0.02389124, -0.02056606,\n",
       "       -0.003208  , -0.00175442, -0.02779999, -0.04809429,  0.01049049,\n",
       "        0.01465398,  0.00538773,  0.01597845,  0.03355633, -0.01048584,\n",
       "       -0.03197055, -0.02174224, -0.04325168, -0.00625255, -0.00358859,\n",
       "        0.02023767,  0.01100595, -0.03110011, -0.03332488, -0.00249746,\n",
       "        0.01253998,  0.02057114,  0.0157886 ,  0.00140111, -0.03751422,\n",
       "       -0.00963631,  0.01219309, -0.00020948, -0.01632159, -0.00150937,\n",
       "        0.0043487 ,  0.04820086,  0.01829152,  0.01443621, -0.023709  ,\n",
       "        0.02098458, -0.01653031,  0.00444767, -0.02905141, -0.03102346,\n",
       "        0.00640802,  0.03971187, -0.03564078, -0.00468435,  0.0290248 ,\n",
       "        0.00139018, -0.02055236,  0.03357238, -0.00272792,  0.0250227 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_vec_mean[0]['vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotions(text_list,lang,emot_vec_mean,model_name):\n",
    "    d2v_model = Doc2Vec.load(model_name)\n",
    "    text_vecs = text2vec(text_list,lang,d2v_model)\n",
    "    emot_scan = [{x['emotion']:1 - spatial.distance.cosine(sample_vec,x['vec']) for x in emot_vec_mean} for sample_vec in text_vecs]\n",
    "    emot_scan1 = [[1 - spatial.distance.cosine(sample_vec,x['vec']) for x in emot_vec_mean] for sample_vec in text_vecs]\n",
    "    emot_pred = [np.argmax(x) for x in emot_scan1]\n",
    "    ranks = [stats.rankdata(x) for x in emot_scan1]\n",
    "    return emot_scan,emot_scan1,emot_pred,ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sent = ['I hate you. You make me so upset.','I love you man. You make me so happy.','I am so unhappy and I cannot go on.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_scan,emot_scan1,emot_pred,ranks = predict_emotions(sample_sent,'en',emot_vec_mean,'emotionsmodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 3],\n",
       " [array([7., 5., 4., 6., 1., 2., 3.]),\n",
       "  array([7., 5., 1., 3., 6., 2., 4.]),\n",
       "  array([1., 2., 3., 7., 5., 6., 4.])])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_pred,ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.26289281,  0.3773383 ,  0.29294954,  0.92111834, -1.59419402,\n",
       "        -1.3346986 ,  0.07459363]),\n",
       " array([ 1.9272342 ,  0.03010905, -1.64427464, -0.08967182,  0.40652682,\n",
       "        -0.63862501,  0.00870141]),\n",
       " array([-1.58932613, -0.87037189, -0.35642639,  1.75559065,  0.41378003,\n",
       "         0.57536677,  0.07138697])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[preprocessing.scale(x) for x in emot_scan1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.993009090423584, 0.9930384159088135, 0.9931976795196533, 0.9932223558425903, 0.9932318925857544, 0.9932933449745178, 0.9933319687843323]\n",
      "[0.9309930205345154, 0.9312282204627991, 0.9313566088676453, 0.9313796162605286, 0.9313846230506897, 0.931472659111023, 0.9318283200263977]\n",
      "[0.9858265519142151, 0.9858867526054382, 0.9859297871589661, 0.9859656095504761, 0.985994279384613, 0.986007809638977, 0.9861066341400146]\n"
     ]
    }
   ],
   "source": [
    "for x in emot_scan:\n",
    "    print(sorted(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_emot = []\n",
    "for x in emot_scan:\n",
    "    item = []\n",
    "    for w in sorted(x, key=x.get, reverse=True):\n",
    "        item.append([w, x[w]])\n",
    "    sort_emot.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['anger', 0.9933319687843323],\n",
       "  ['guilt', 0.9932933449745178],\n",
       "  ['disgust', 0.9932318925857544],\n",
       "  ['fear', 0.9932223558425903],\n",
       "  ['shame', 0.9931976795196533],\n",
       "  ['sadness', 0.9930384159088135],\n",
       "  ['joy', 0.993009090423584]],\n",
       " [['anger', 0.9318283200263977],\n",
       "  ['joy', 0.931472659111023],\n",
       "  ['disgust', 0.9313846230506897],\n",
       "  ['shame', 0.9313796162605286],\n",
       "  ['guilt', 0.9313566088676453],\n",
       "  ['sadness', 0.9312282204627991],\n",
       "  ['fear', 0.9309930205345154]],\n",
       " [['guilt', 0.9861066341400146],\n",
       "  ['sadness', 0.986007809638977],\n",
       "  ['joy', 0.985994279384613],\n",
       "  ['shame', 0.9859656095504761],\n",
       "  ['fear', 0.9859297871589661],\n",
       "  ['disgust', 0.9858867526054382],\n",
       "  ['anger', 0.9858265519142151]]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emot_scan[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
